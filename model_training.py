# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jT96AG67EOuDs-V818HWgiyKSs6Zr_D
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import warnings
warnings.filterwarnings("ignore")

# Load dataset
df = pd.read_csv(r"D:\ProjectResearch\Bengaluru_House_Data.csv")
df.head()

# Data Preprocessing
df.drop(['society', 'availability', 'balcony'], axis=1, inplace=True)
df['total_sqft'] = df['total_sqft'].apply(lambda x: str(x).split('-')[0] if '-' in str(x) else x)
df['total_sqft'] = pd.to_numeric(df['total_sqft'], errors='coerce')

# Impute missing values
df['total_sqft'].fillna(df['total_sqft'].median(), inplace=True)
df['bath'].fillna(df['bath'].median(), inplace=True)
df['location'].fillna(df['location'].mode()[0], inplace=True)
df['area_type'].fillna(df['area_type'].mode()[0], inplace=True)
df['size'].fillna(df['size'].mode()[0], inplace=True)

# Feature Engineering
if 'size' in df.columns:
    df['bhk'] = df['size'].apply(lambda x: int(str(x).split(' ')[0]) if pd.notnull(x) else 0)
    df.drop(['size'], axis=1, inplace=True)
else:
    df['bhk'] = 0  # fallback if 'size' column is missing

# Handle potential division by zero and missing values
# Replace 0 bhk to avoid division error
df['bhk'] = df['bhk'].replace(0, 1)

# Calculate price per sqft and bath per bhk safely
df['price_per_sqft'] = df['price'] * 100000 / df['total_sqft']
df['bath_per_bhk'] = df['bath'] / df['bhk']

# Handle infinities and missing values
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(df.median(numeric_only=True), inplace=True)

# Outlier Removal
df = df[df['total_sqft'] / df['bhk'] >= 300]
df = df[df['price_per_sqft'] < df['price_per_sqft'].quantile(0.99)]

# Encode categorical features
label_encoder_location = LabelEncoder()
df['location'] = label_encoder_location.fit_transform(df['location'].astype(str))
label_encoder_area = LabelEncoder()
df['area_type'] = label_encoder_area.fit_transform(df['area_type'].astype(str))

# Define features and target
X = df.drop(['price'], axis=1)
y = df['price']

# Check data types before scaling
print("X dtypes before scaling:\n", X.dtypes)

# Train-test split and scaling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# XGBoost model (updated parameters for improved accuracy)
xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=7, subsample=0.9, colsample_bytree=0.8, random_state=42)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb = xgb_model.predict(X_test_scaled)
acc_xgb = r2_score(y_test, y_pred_xgb) * 100
print(f"XGBoost Accuracy: {acc_xgb:.2f}%")

# ANN Model (fine-tuned)
ann_model = Sequential([
    Dense(256, activation='relu', input_dim=X_train_scaled.shape[1]),
    Dropout(0.2),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(1)
])
ann_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
early_stop_ann = EarlyStopping(patience=10, restore_best_weights=True)
ann_model.fit(X_train_scaled, y_train, epochs=150, batch_size=32, verbose=0, callbacks=[early_stop_ann])
y_pred_ann = ann_model.predict(X_test_scaled).flatten()
acc_ann = r2_score(y_test, y_pred_ann) * 100
print(f"ANN Accuracy: {acc_ann:.2f}%")
# LSTM Model (fine-tuned)
X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

lstm_model = Sequential([
    LSTM(128, activation='tanh', return_sequences=True, input_shape=(X_train_scaled.shape[1], 1)),
    Dropout(0.2),
    LSTM(64, activation='tanh'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])
lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
early_stop_lstm = EarlyStopping(patience=10, restore_best_weights=True)
lstm_model.fit(X_train_lstm, y_train, epochs=150, batch_size=32, verbose=0, callbacks=[early_stop_lstm])
y_pred_lstm = lstm_model.predict(X_test_lstm).flatten()
acc_lstm = r2_score(y_test, y_pred_lstm) * 100
print(f"LSTM Accuracy: {acc_lstm:.2f}%")

# Weighted Hybrid Model
# Weighted Hybrid Model
total_score = acc_xgb + acc_ann + acc_lstm
w_xgb = acc_xgb / total_score
w_ann = acc_ann / total_score
w_lstm = acc_lstm / total_score

hybrid_pred = (w_xgb * y_pred_xgb + w_ann * y_pred_ann + w_lstm * y_pred_lstm)
hybrid_acc = r2_score(y_test, hybrid_pred) * 100
print(f"Hybrid Accuracy (Score Proportional Weights): {hybrid_acc:.2f}%")

# Save models
joblib.dump(scaler, "scaler.pkl")
joblib.dump(xgb_model, "xgb_model.pkl")
ann_model.save("ann_model.h5")
lstm_model.save("lstm_model.h5")
print("All models saved successfully.")

# SHAP Explainability for XGBoost
import shap
import matplotlib.pyplot as plt
explainer_xgb = shap.Explainer(xgb_model, X_train_scaled)
shap_values_xgb = explainer_xgb(X_train_scaled)
shap.summary_plot(shap_values_xgb, X_train_scaled, feature_names=X.columns, show=False)
plt.savefig("shap_summary_plot_xgb.png")

# SHAP Explainability for ANN
explainer_ann = shap.DeepExplainer(ann_model, X_train_scaled[:100])
shap_values_ann = explainer_ann.shap_values(X_train_scaled[:100])

# Reshape shap_values_ann if necessary
# If shap_values_ann is a list of arrays, select the appropriate one
# Assuming your model has a single output, you likely need shap_values_ann[0]
if isinstance(shap_values_ann, list):
    shap_values_ann = shap_values_ann[0]

# Ensure shap_values_ann has the correct shape: (num_instances, num_features)
shap_values_ann = shap_values_ann.reshape(X_train_scaled[:100].shape)  # Reshape to match X_train_scaled[:100]

shap.summary_plot(shap_values_ann, X_train_scaled[:100], feature_names=X.columns, show=False)
plt.savefig("shap_summary_plot_ann.png")

# SHAP for LSTM using 2D input workaround (experimental)
X_sample = X_train_scaled[:50]  # Use 2D input for KernelExplainer
explainer_lstm = shap.KernelExplainer(lambda x: lstm_model.predict(x.reshape(x.shape[0], x.shape[1], 1)), X_sample)
shap_values_lstm = explainer_lstm.shap_values(X_sample)
shap.summary_plot(shap_values_lstm, X_sample, feature_names=X.columns, show=False)
plt.savefig("shap_summary_plot_lstm.png")

# SHAP for LSTM
try:
    background_lstm = X_train_scaled[np.random.choice(X_train_scaled.shape[0], 50, replace=False)]
    explainer_lstm = shap.KernelExplainer(lambda x: lstm_model.predict(x.reshape((x.shape[0], x.shape[1], 1))).flatten(), background_lstm)
    shap_values_lstm = explainer_lstm.shap_values(X_train_scaled[:50])
    shap.summary_plot(shap_values_lstm, X_train_scaled[:50], feature_names=X.columns, show=False)
    plt.savefig("shap_summary_plot_lstm.png")
except Exception as e:
    print(f"SHAP LSTM explanation failed: {e}")

# Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.tight_layout()
plt.savefig("correlation_heatmap.png")

plt.figure(figsize=(10, 6))
sns.histplot(df['price'], bins=50, kde=True)
plt.title("Price Distribution")
plt.xlabel("Price (in Lakhs)")
plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig("price_distribution.png")

plt.figure(figsize=(10, 6))
sns.boxplot(x='bhk', y='price', data=df)
plt.title("BHK vs Price")
plt.tight_layout()
plt.savefig("bhk_vs_price_boxplot.png")

